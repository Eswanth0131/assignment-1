{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations\n",
    "---\n",
    "## Log-likelihood\n",
    "For data points $x_1, \\dots, x_m \\in \\mathbb{R}$ from a mixture of 2 one-dimensional Gaussians,  \n",
    "$$\n",
    "p(x_i) = \\pi_1 \\,\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\,\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2),\n",
    "$$  \n",
    "where $\\pi_1, \\pi_2 \\geq 0$, $\\pi_1 + \\pi_2 = 1$, and $\\sigma_1^2, \\sigma_2^2 > 0$.  \n",
    "The log-likelihood is  \n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{i=1}^m \\log \\Big( \\pi_1 \\,\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\,\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\Big).\n",
    "$$  \n",
    "Explanation: The log converts the product of probabilities into a sum, which is easier to optimize and more numerically stable.  \n",
    "\n",
    "---\n",
    "## Responsibility\n",
    "Define the responsibility of component $k \\in \\{1,2\\}$ for point $x_i$:  \n",
    "$$\n",
    "r_{ik} = \\frac{\\pi_k \\,\\mathcal{N}(x_i \\mid \\mu_k, \\sigma_k^2)}\n",
    "{\\pi_1 \\,\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\,\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)}.\n",
    "$$  \n",
    "These satisfy  \n",
    "$$\n",
    "r_{i1} + r_{i2} = 1 \\quad \\text{for each } i.\n",
    "$$  \n",
    "Explanation: Responsibilities are assignments telling us the probability that each Gaussian component generated point $x_i$.  \n",
    "\n",
    "---\n",
    "## Gradients\n",
    "Let  \n",
    "$$\n",
    "N_k = \\sum_{i=1}^m r_{ik}\n",
    "$$  \n",
    "be the effective number of points assigned to component $k$.  \n",
    "\n",
    "---\n",
    "### Means\n",
    "The gradient with respect to the means is  \n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\mu_k} = \\sum_{i=1}^m r_{ik} \\,\\frac{(x_i - \\mu_k)}{\\sigma_k^2}, \\quad k \\in \\{1,2\\}.\n",
    "$$  \n",
    "Explanation: This moves $\\mu_k$ closer to the weighted average of the data points, with the responsibilities giving weights.\n",
    "\n",
    "---\n",
    "### Variances\n",
    "For the 1-dimensional variances $\\sigma_k^2$, the gradient is  \n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma_k^2}\n",
    "= \\frac{1}{2} \\sum_{i=1}^m r_{ik} \\left[ \\frac{(x_i - \\mu_k)^2}{(\\sigma_k^2)^2} - \\frac{1}{\\sigma_k^2} \\right], \\quad k \\in \\{1,2\\}.\n",
    "$$  \n",
    "Explanation: The variance increases if points are far from the mean and decreases if they are too close, balancing the spread.  \n",
    "\n",
    "---\n",
    "### Mixing weights\n",
    "The gradient of the log-likelihood with respect to the mixing weights is  \n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\pi_k} = \\frac{N_k}{\\pi_k}, \\quad k \\in \\{1,2\\}.\n",
    "$$  \n",
    "Explanation: Each $\\pi_k$ is updated in proportion to the effective number of points assigned to its component.  \n",
    "Since $\\pi_1 + \\pi_2 = 1$, we typically reparameterize with softmax: $\\pi_k = \\text{softmax}(\\alpha_k)$.  \n",
    "\n",
    "---\n",
    "We derived the formulas needed to update the mixing weights, means, and variances for a 2-component, 1-dimensional Gaussian mixture model. These results are exactly what is required to solve the estimation problem in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
